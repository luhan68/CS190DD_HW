{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"HW4.ipynb","provenance":[]}},"cells":[{"source":["### Problem 1"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"cHSaycfVdHRJ"},"source":["### Problem 2\n","\n","Experimenting with **k-anomity, i-diversity, and t-closeness**. \n","\n","Consider a dataset, for example, with 3 ordinary attributes and 1 sensitive attribute. Let the 3 ordinary attributes be Age, Sex, and Education and the sensitive attribute be Income, each row in this dataset is of the form:\n","\n","$$\n","    [Age, Sex, Education, Income]\n","$$\n","\n","A hacker is interested in knowing the sensitive attribute Income. When the dataset is designed so that if complies with either **k-anomity**, **i-diversity**, and/or **t-closeness**, even if he or she somehow figures out the values of the three, the hacker may not retrive the sensitive information accurately. In general, **k-anomity** is weaker than **i-diversity**, which, in turn, is weaker than **t-closeness**.\n","\n","By definition, **k-anomity** means that there is at least **k** different rows in the table of which ordinary values are a particular combination of Age, Sex, and Education. For example, the hacker knows the information of the person of interest is Age = 31, Sex = Female, and Education = BS. He or she looks into the data table and found that there are 3 rows with that combination:\n","\n","$$\n","    [Age=31, Sex=Female, Education=BS, Income=300k]\n","$$\n","$$\n","    [Age=31, Sex=Female, Education=BS, Income=70k]\n","$$\n","$$\n","    [Age=31, Sex=Female, Education=BS, Income=20k]\n","$$\n","\n","The hacker cannot tell accurately what the income of the person is because it can be one of the 3 values shown. This particular combination of information has 3-anomity. If every combination corresponds to at least 3 rows, then the dataset has 3-anomity.\n","\n","a) Let's look at the dataset **\"table3.csv\"**, a simplified version of **\"table1.csv\"** from problem 1. Let the sensitive attribute be **education** and others be ordinary attributes. Calculate the anomity of the dataset (the value **k**). First, find all the posible combinations of the ordinary attributes that exists in the dataset. After that, determine the anomity for each combination. The anomity of the dataset is the smallest anomity among the combinations."]},{"cell_type":"code","metadata":{"id":"Qrm7S5y5dHRJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ixGLowBjdHRK"},"source":["We can improve the **k-anomity** of the dataset by \"suppressing\" the ordinary attributes. Suppressing means reducing the resolution of the attribute's value. For this problem, let's suppress Age by replacing the exact age with an age range. For example, instead of leaving age = 32, replace it with age = 30-40. Apply this to **\"table3.csv\"** with the ranges {<20, 20-30, 30-50, >50}. Check if the anomity improves. "]},{"cell_type":"code","metadata":{"id":"AGqI5mltdHRK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pv-RmXqCdHRK"},"source":["**K-anomity** is nice, however, it fails in many cases. If the rows which share a combination of ordinary attributes have only a few values for the sensitive attribute, then it is not much better than having no anomity at all. For example, consider:\n","\n","$$\n","    [Age=31, Sex=Female, Education=BS, Income=300k]\n","$$\n","$$\n","    [Age=31, Sex=Female, Education=BS, Income=20k]\n","$$\n","$$\n","    [Age=31, Sex=Female, Education=BS, Income=20k]\n","$$\n","$$\n","    [Age=31, Sex=Female, Education=BS, Income=20k]\n","$$\n","\n","When **k-anomity** fails in the second case, **i-diversity** comes to the rescue. **I-diversity** states that the rows of a particular combination of information must have at least i different values for the sensitive attribute. The above example has 2-diversity, which is not good. \n","\n","b) Calculate the diversity of the dataset **\"table3.csv\"**. Follow similar steps as in part a. "]},{"cell_type":"code","metadata":{"id":"eZYCvcukdHRK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6qWt9JvTdHRK"},"source":["Suppressing an attribute can also improve the **i-diversity** of the dataset. Repeat the suppression as in **part a** and check if the diversity improves. If it does not, consider further suppress age by using the range {<20, 20-50, >50}."]},{"cell_type":"code","metadata":{"id":"FFVPKWrodHRK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TX1lsm8ydHRK"},"source":["**T-closeness** is even better than **i-diversity**. **T-closeness** requires that for every combination of information, the distribution of the sensitive attribute's value among the corresponding rows must be close to the overall distribution of the sensitive attribute's value for the whole dataset. Distance between distribution is calculated using the Earth Mover Distance (EMD). The dataset has **t-closeness** if no distance exceeds **t**. \n","\n","c) Calculate the overall distribution of **education**. Find the **t-closeness** of the dataset (largest distance between any combination's distribution of marital-status and the overall distribution).\n","\n","You can use **scipy.stats.wasserstein_distance** to calculate the EMD."]},{"cell_type":"code","metadata":{"id":"BjR4iG2bdHRK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W5LHcufCdHRK"},"source":["### Problem 4 \n","\n","There are 2 regression datasets given to you: \"group1.csv\" and \"group2.csv\". Both have 2 attributes and no label. Load them and store them in $X_1$ and $X_2$, respectively. "]},{"cell_type":"code","metadata":{"id":"zpMsZXkPdHRK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AiDGJzt-dHRK"},"source":["a) Run Linear Regression on each of the datasets. Are the coefficients positive or negative? Provide a plot for each dataset. "]},{"cell_type":"code","metadata":{"id":"C4AhAsWxdHRK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j1PyBaU1dHRL"},"source":["b) Now combine both datasets into a single large dataset. Call this dataset $X$ ($X=X_1 \\cup X_2$). Again, run Linear Regression on the combined dataset $X$. Is the coefficient positive or negative? Provide a plot. "]},{"cell_type":"code","metadata":{"id":"9BjTQh9BdHRL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RTV1WfcWdHRL"},"source":["c) What is the name of this illustrated paradox? What do the above results tell us about modeling the relationship between two variables in the presence of a missing attribute? To give you some intuition, imagine there is a third unobserved attribute $Z$ that has different values depending on which group an example belongs to. In other words, every data point in $X_1$ has $Z=1$ and every data point in $X_2$ has $Z=2$. Attribute $Z$ essentially partitions the whole dataset $X$ into 2 subsets $X_1$ and $X_2$.  "]},{"cell_type":"code","metadata":{"id":"8VvUS2pXdHRL"},"source":[""],"execution_count":null,"outputs":[]}]}